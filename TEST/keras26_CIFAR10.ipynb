{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras26_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdgus1514/ML/blob/master/keras26_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96kWnCvqEZF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5fe01a0-469f-4e13-920c-ec307ae62b80"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 3채널 구성된 32x32 이미지 6만장\n",
        "IMG_CHANNELS = 3\n",
        "IMG_ROWS = 32\n",
        "IMG_CLOS = 32\n",
        "\n",
        "\n",
        "\n",
        "# 상수 정의\n",
        "BATCH_SIZE = 200\n",
        "NB_EPOCH = 1000\n",
        "NB_CLASSES = 10\n",
        "VERBOSE = 1\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "print(\"X_train shape : \", X_train.shape)\n",
        "print(X_train.shape[0], \"train samples\")\n",
        "print(X_test.shape[0], \"test samples\")\n",
        "\n",
        "# 범주형으로 변환\n",
        "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# 실수형으로 변환 및 정규화\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "# 신경망 정의\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=(IMG_ROWS, IMG_CLOS, IMG_CHANNELS)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512, (3,3), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3,3), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(512))\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "# 학습\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIM, metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=20)    # 변화값이 patience이상 변경 없을경우 중지\n",
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE, callbacks=[early_stopping_callback])\n",
        "\n",
        "print(\"Testing...\")\n",
        "\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
        "print(\"\\nTest score : \", score[0])\n",
        "print(\"Test accuracy : \", score[1])\n",
        "\n",
        "\n",
        "'''\n",
        "# 히스토리에 있는 모든 데이터 나열\n",
        "print(history.history.keys())\n",
        "\n",
        "\n",
        "# 시각화\n",
        "plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"val_acc\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"val_acc\"])\n",
        "plt.title(\"model loss, accuracy\")\n",
        "plt.ylabel(\"loss, acc\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.legend([\"train loss\", \"test loss\", \"train acc\", \"test acc\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape :  (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1000\n",
            "40000/40000 [==============================] - 16s 397us/step - loss: 3.0912 - acc: 0.3122 - val_loss: 2.1250 - val_acc: 0.3557\n",
            "Epoch 2/1000\n",
            "40000/40000 [==============================] - 13s 334us/step - loss: 1.6818 - acc: 0.4701 - val_loss: 1.7038 - val_acc: 0.4600\n",
            "Epoch 3/1000\n",
            "40000/40000 [==============================] - 13s 335us/step - loss: 1.4315 - acc: 0.5490 - val_loss: 1.3363 - val_acc: 0.5632\n",
            "Epoch 4/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 1.2174 - acc: 0.6019 - val_loss: 1.2398 - val_acc: 0.6126\n",
            "Epoch 5/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 1.0883 - acc: 0.6369 - val_loss: 1.0423 - val_acc: 0.6470\n",
            "Epoch 6/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 1.0031 - acc: 0.6590 - val_loss: 0.8871 - val_acc: 0.7029\n",
            "Epoch 7/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.9162 - acc: 0.6836 - val_loss: 0.7792 - val_acc: 0.7270\n",
            "Epoch 8/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.8572 - acc: 0.7028 - val_loss: 0.7695 - val_acc: 0.7387\n",
            "Epoch 9/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.8105 - acc: 0.7167 - val_loss: 0.7095 - val_acc: 0.7538\n",
            "Epoch 10/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.7757 - acc: 0.7298 - val_loss: 0.6818 - val_acc: 0.7678\n",
            "Epoch 11/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.7375 - acc: 0.7400 - val_loss: 0.7236 - val_acc: 0.7528\n",
            "Epoch 12/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.7179 - acc: 0.7503 - val_loss: 0.6800 - val_acc: 0.7679\n",
            "Epoch 13/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.6928 - acc: 0.7577 - val_loss: 0.7126 - val_acc: 0.7635\n",
            "Epoch 14/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.6698 - acc: 0.7657 - val_loss: 0.6310 - val_acc: 0.7831\n",
            "Epoch 15/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.6473 - acc: 0.7729 - val_loss: 0.5858 - val_acc: 0.7983\n",
            "Epoch 16/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.6244 - acc: 0.7778 - val_loss: 0.6223 - val_acc: 0.7870\n",
            "Epoch 17/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.6149 - acc: 0.7837 - val_loss: 0.6338 - val_acc: 0.7841\n",
            "Epoch 18/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.6038 - acc: 0.7883 - val_loss: 0.6072 - val_acc: 0.7947\n",
            "Epoch 19/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5855 - acc: 0.7964 - val_loss: 0.5926 - val_acc: 0.7956\n",
            "Epoch 20/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5820 - acc: 0.7961 - val_loss: 0.5880 - val_acc: 0.8009\n",
            "Epoch 21/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5655 - acc: 0.8022 - val_loss: 0.6031 - val_acc: 0.7956\n",
            "Epoch 22/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.5599 - acc: 0.8051 - val_loss: 0.6149 - val_acc: 0.7912\n",
            "Epoch 23/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5381 - acc: 0.8103 - val_loss: 0.5440 - val_acc: 0.8181\n",
            "Epoch 24/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5257 - acc: 0.8150 - val_loss: 0.6180 - val_acc: 0.7886\n",
            "Epoch 25/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5227 - acc: 0.8177 - val_loss: 0.6138 - val_acc: 0.7943\n",
            "Epoch 26/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.5089 - acc: 0.8195 - val_loss: 0.6051 - val_acc: 0.7963\n",
            "Epoch 27/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.5007 - acc: 0.8253 - val_loss: 0.5778 - val_acc: 0.8004\n",
            "Epoch 28/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4897 - acc: 0.8276 - val_loss: 0.5712 - val_acc: 0.8033\n",
            "Epoch 29/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4871 - acc: 0.8290 - val_loss: 0.5758 - val_acc: 0.8039\n",
            "Epoch 30/1000\n",
            "40000/40000 [==============================] - 13s 331us/step - loss: 0.4752 - acc: 0.8337 - val_loss: 0.6217 - val_acc: 0.7937\n",
            "Epoch 31/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4691 - acc: 0.8371 - val_loss: 0.5133 - val_acc: 0.8243\n",
            "Epoch 32/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4626 - acc: 0.8365 - val_loss: 0.5075 - val_acc: 0.8282\n",
            "Epoch 33/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4543 - acc: 0.8413 - val_loss: 0.5196 - val_acc: 0.8238\n",
            "Epoch 34/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.4505 - acc: 0.8423 - val_loss: 0.4990 - val_acc: 0.8334\n",
            "Epoch 35/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4381 - acc: 0.8461 - val_loss: 0.5409 - val_acc: 0.8148\n",
            "Epoch 36/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4246 - acc: 0.8519 - val_loss: 0.5159 - val_acc: 0.8224\n",
            "Epoch 37/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.4267 - acc: 0.8494 - val_loss: 0.5488 - val_acc: 0.8185\n",
            "Epoch 38/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4205 - acc: 0.8505 - val_loss: 0.5255 - val_acc: 0.8281\n",
            "Epoch 39/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4150 - acc: 0.8541 - val_loss: 0.4759 - val_acc: 0.8364\n",
            "Epoch 40/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.4113 - acc: 0.8541 - val_loss: 0.4840 - val_acc: 0.8382\n",
            "Epoch 41/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3998 - acc: 0.8597 - val_loss: 0.5616 - val_acc: 0.8171\n",
            "Epoch 42/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.3954 - acc: 0.8594 - val_loss: 0.5258 - val_acc: 0.8283\n",
            "Epoch 43/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3962 - acc: 0.8596 - val_loss: 0.5420 - val_acc: 0.8211\n",
            "Epoch 44/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3923 - acc: 0.8609 - val_loss: 0.4732 - val_acc: 0.8432\n",
            "Epoch 45/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3826 - acc: 0.8649 - val_loss: 0.4973 - val_acc: 0.8322\n",
            "Epoch 46/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3838 - acc: 0.8645 - val_loss: 0.5995 - val_acc: 0.8124\n",
            "Epoch 47/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3738 - acc: 0.8683 - val_loss: 0.5178 - val_acc: 0.8316\n",
            "Epoch 48/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.3709 - acc: 0.8685 - val_loss: 0.4859 - val_acc: 0.8386\n",
            "Epoch 49/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3691 - acc: 0.8707 - val_loss: 0.4780 - val_acc: 0.8447\n",
            "Epoch 50/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3599 - acc: 0.8737 - val_loss: 0.4831 - val_acc: 0.8387\n",
            "Epoch 51/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3542 - acc: 0.8745 - val_loss: 0.4892 - val_acc: 0.8353\n",
            "Epoch 52/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3502 - acc: 0.8760 - val_loss: 0.4591 - val_acc: 0.8469\n",
            "Epoch 53/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3556 - acc: 0.8740 - val_loss: 0.4856 - val_acc: 0.8380\n",
            "Epoch 54/1000\n",
            "40000/40000 [==============================] - 13s 331us/step - loss: 0.3437 - acc: 0.8763 - val_loss: 0.4664 - val_acc: 0.8432\n",
            "Epoch 55/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3417 - acc: 0.8802 - val_loss: 0.4770 - val_acc: 0.8447\n",
            "Epoch 56/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3352 - acc: 0.8816 - val_loss: 0.6014 - val_acc: 0.8092\n",
            "Epoch 57/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3318 - acc: 0.8816 - val_loss: 0.4778 - val_acc: 0.8435\n",
            "Epoch 58/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3300 - acc: 0.8824 - val_loss: 0.4816 - val_acc: 0.8425\n",
            "Epoch 59/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3255 - acc: 0.8844 - val_loss: 0.4571 - val_acc: 0.8485\n",
            "Epoch 60/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3231 - acc: 0.8859 - val_loss: 0.5008 - val_acc: 0.8359\n",
            "Epoch 61/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3212 - acc: 0.8841 - val_loss: 0.5175 - val_acc: 0.8309\n",
            "Epoch 62/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.3229 - acc: 0.8858 - val_loss: 0.4666 - val_acc: 0.8439\n",
            "Epoch 63/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3194 - acc: 0.8860 - val_loss: 0.4887 - val_acc: 0.8429\n",
            "Epoch 64/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3168 - acc: 0.8874 - val_loss: 0.4970 - val_acc: 0.8394\n",
            "Epoch 65/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3071 - acc: 0.8918 - val_loss: 0.4950 - val_acc: 0.8438\n",
            "Epoch 66/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3044 - acc: 0.8915 - val_loss: 0.4551 - val_acc: 0.8486\n",
            "Epoch 67/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3083 - acc: 0.8898 - val_loss: 0.4688 - val_acc: 0.8484\n",
            "Epoch 68/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.2991 - acc: 0.8924 - val_loss: 0.4656 - val_acc: 0.8533\n",
            "Epoch 69/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3003 - acc: 0.8917 - val_loss: 0.4522 - val_acc: 0.8558\n",
            "Epoch 70/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.3002 - acc: 0.8926 - val_loss: 0.4605 - val_acc: 0.8498\n",
            "Epoch 71/1000\n",
            "40000/40000 [==============================] - 13s 331us/step - loss: 0.2930 - acc: 0.8949 - val_loss: 0.5117 - val_acc: 0.8377\n",
            "Epoch 72/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2896 - acc: 0.8953 - val_loss: 0.4996 - val_acc: 0.8388\n",
            "Epoch 73/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2880 - acc: 0.8978 - val_loss: 0.5077 - val_acc: 0.8402\n",
            "Epoch 74/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2886 - acc: 0.8978 - val_loss: 0.4973 - val_acc: 0.8398\n",
            "Epoch 75/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2840 - acc: 0.8991 - val_loss: 0.4495 - val_acc: 0.8554\n",
            "Epoch 76/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2789 - acc: 0.9017 - val_loss: 0.4997 - val_acc: 0.8416\n",
            "Epoch 77/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2760 - acc: 0.9019 - val_loss: 0.4507 - val_acc: 0.8535\n",
            "Epoch 78/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2822 - acc: 0.8988 - val_loss: 0.4561 - val_acc: 0.8531\n",
            "Epoch 79/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2804 - acc: 0.9006 - val_loss: 0.4919 - val_acc: 0.8415\n",
            "Epoch 80/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2705 - acc: 0.9040 - val_loss: 0.4679 - val_acc: 0.8522\n",
            "Epoch 81/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2710 - acc: 0.9020 - val_loss: 0.4716 - val_acc: 0.8515\n",
            "Epoch 82/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2676 - acc: 0.9060 - val_loss: 0.4670 - val_acc: 0.8490\n",
            "Epoch 83/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2703 - acc: 0.9030 - val_loss: 0.4586 - val_acc: 0.8511\n",
            "Epoch 84/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2600 - acc: 0.9078 - val_loss: 0.5394 - val_acc: 0.8327\n",
            "Epoch 85/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2676 - acc: 0.9033 - val_loss: 0.5212 - val_acc: 0.8351\n",
            "Epoch 86/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2655 - acc: 0.9056 - val_loss: 0.4883 - val_acc: 0.8436\n",
            "Epoch 87/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2587 - acc: 0.9071 - val_loss: 0.5031 - val_acc: 0.8434\n",
            "Epoch 88/1000\n",
            "40000/40000 [==============================] - 13s 333us/step - loss: 0.2560 - acc: 0.9078 - val_loss: 0.4631 - val_acc: 0.8528\n",
            "Epoch 89/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2574 - acc: 0.9080 - val_loss: 0.5063 - val_acc: 0.8408\n",
            "Epoch 90/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2550 - acc: 0.9088 - val_loss: 0.4679 - val_acc: 0.8534\n",
            "Epoch 91/1000\n",
            "40000/40000 [==============================] - 13s 331us/step - loss: 0.2574 - acc: 0.9074 - val_loss: 0.4664 - val_acc: 0.8519\n",
            "Epoch 92/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2588 - acc: 0.9081 - val_loss: 0.5322 - val_acc: 0.8329\n",
            "Epoch 93/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2535 - acc: 0.9087 - val_loss: 0.4511 - val_acc: 0.8522\n",
            "Epoch 94/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2528 - acc: 0.9096 - val_loss: 0.4775 - val_acc: 0.8522\n",
            "Epoch 95/1000\n",
            "40000/40000 [==============================] - 13s 332us/step - loss: 0.2417 - acc: 0.9137 - val_loss: 0.4680 - val_acc: 0.8503\n",
            "Testing...\n",
            "10000/10000 [==============================] - 1s 102us/step\n",
            "\n",
            "Test score :  0.48254425823688507\n",
            "Test accuracy :  0.8465000021457673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# 히스토리에 있는 모든 데이터 나열\\nprint(history.history.keys())\\n\\n\\n# 시각화\\nplt.plot(history.history[\"acc\"])\\nplt.plot(history.history[\"val_acc\"])\\nplt.title(\"model accuracy\")\\nplt.ylabel(\"accuracy\")\\nplt.xlabel(\"epoch\")\\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\\nplt.show()\\n\\n\\nplt.plot(history.history[\"loss\"])\\nplt.plot(history.history[\"val_loss\"])\\nplt.title(\"model loss\")\\nplt.ylabel(\"loss\")\\nplt.xlabel(\"epoch\")\\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\\nplt.show()\\n\\n\\nplt.plot(history.history[\"loss\"])\\nplt.plot(history.history[\"val_loss\"])\\nplt.plot(history.history[\"acc\"])\\nplt.plot(history.history[\"val_acc\"])\\nplt.title(\"model loss, accuracy\")\\nplt.ylabel(\"loss, acc\")\\nplt.xlabel(\"epochs\")\\nplt.legend([\"train loss\", \"test loss\", \"train acc\", \"test acc\"], loc=\"upper left\")\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}