{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdgus1514/ML/blob/master/colab_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdB0iRSjSE9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "81eb7681-2bc2-40c7-e02b-d5f236f6e06f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "## 1. 학습데이터\n",
        "x = np.array([range(1000), range(3110,4110), range(1000)])\n",
        "y = np.array([range(5010, 6010)])\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# 차원변경\n",
        "x = np.transpose(x)\n",
        "y = np.transpose(y)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 분할( 6/2/2)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=66, test_size=0.4)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, random_state=66, test_size=0.5)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(x_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 2. 모델구성(레이어, 노드 개수 설정)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Dense(100, input_shape=(3, ), activation=\"relu\"))\n",
        "model.add(Dense(1000, input_shape=(3, ), activation=\"relu\", kernel_regularizer=regularizers.l1(0.05)))  # Lasso regression\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(1000, kernel_regularizer=regularizers.l2(0.05)))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(1000))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(1000))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(1000, kernel_regularizer=regularizers.l2(0.05)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 3. 훈련\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=10, validation_data=(x_val, y_val))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 4. 평가예측\n",
        "loss, acc = model.evaluate(x_test, y_test, batch_size=10)\n",
        "print(\"acc : \", acc)\n",
        "\n",
        "y_predict = model.predict(x_test)\n",
        "print(y_predict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## RMSE 구하기 (오차비교)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def RMSE(y_test, y_predict):    # 결과값 :? 예측값\n",
        "    return np.sqrt(mean_squared_error(y_test, y_predict))\n",
        "\n",
        "print(\"RMSE : \", RMSE(y_test, y_predict))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## R2 구하기 (결정계수 >> 1에 가까울수록 좋음)\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2_y_predict = r2_score(y_test, y_predict)\n",
        "print(\"R2 : \", r2_y_predict)\n",
        "\n",
        "print(\"loss : \", loss)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 1000)\n",
            "(1, 1000)\n",
            "(1000, 3)\n",
            "(1000, 1)\n",
            "(600, 3)\n",
            "(200, 3)\n",
            "(200, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 01:11:54.112374 140462427150208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0729 01:11:54.131330 140462427150208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0729 01:11:54.134420 140462427150208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0729 01:11:54.231399 140462427150208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0729 01:11:54.259475 140462427150208 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0729 01:11:54.260807 140462427150208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0729 01:11:54.300288 140462427150208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0729 01:11:54.334852 140462427150208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0729 01:11:54.369946 140462427150208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0729 01:11:54.491903 140462427150208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0729 01:11:54.525659 140462427150208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0729 01:11:55.107931 140462427150208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 600 samples, validate on 200 samples\n",
            "Epoch 1/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 30325987.5333 - mean_squared_error: 30325886.4667 - val_loss: 30745656.8000 - val_mean_squared_error: 30745554.8000\n",
            "Epoch 2/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 30298897.2667 - mean_squared_error: 30298792.1000 - val_loss: 30765235.9000 - val_mean_squared_error: 30765127.9000\n",
            "Epoch 3/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 30250356.2333 - mean_squared_error: 30250246.8333 - val_loss: 30700695.7000 - val_mean_squared_error: 30700583.7000\n",
            "Epoch 4/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 30169521.3667 - mean_squared_error: 30169406.5333 - val_loss: 30612742.3000 - val_mean_squared_error: 30612626.3000\n",
            "Epoch 5/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 30046225.4000 - mean_squared_error: 30046106.6000 - val_loss: 30466631.6000 - val_mean_squared_error: 30466511.6000\n",
            "Epoch 6/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 29842298.4000 - mean_squared_error: 29842174.2000 - val_loss: 30228616.7000 - val_mean_squared_error: 30228488.7000\n",
            "Epoch 7/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 29577539.6667 - mean_squared_error: 29577409.8000 - val_loss: 29709031.9000 - val_mean_squared_error: 29708899.9000\n",
            "Epoch 8/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 29183776.0667 - mean_squared_error: 29183640.1000 - val_loss: 29280429.6000 - val_mean_squared_error: 29280289.6000\n",
            "Epoch 9/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 28696968.1333 - mean_squared_error: 28696825.9000 - val_loss: 28531543.6000 - val_mean_squared_error: 28531397.6000\n",
            "Epoch 10/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 28101572.8667 - mean_squared_error: 28101423.3333 - val_loss: 28065091.4000 - val_mean_squared_error: 28064937.4000\n",
            "Epoch 11/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 27371653.2000 - mean_squared_error: 27371497.3667 - val_loss: 27202963.8000 - val_mean_squared_error: 27202803.8000\n",
            "Epoch 12/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 26517238.8000 - mean_squared_error: 26517075.4333 - val_loss: 26147186.6000 - val_mean_squared_error: 26147020.6000\n",
            "Epoch 13/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 25577540.9333 - mean_squared_error: 25577370.1667 - val_loss: 25278351.1000 - val_mean_squared_error: 25278177.1000\n",
            "Epoch 14/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 24541034.3667 - mean_squared_error: 24540855.9667 - val_loss: 24091006.6000 - val_mean_squared_error: 24090824.6000\n",
            "Epoch 15/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 23417550.4333 - mean_squared_error: 23417363.9667 - val_loss: 22783431.9000 - val_mean_squared_error: 22783241.9000\n",
            "Epoch 16/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 22295225.7333 - mean_squared_error: 22295031.7333 - val_loss: 21702334.3000 - val_mean_squared_error: 21702136.3000\n",
            "Epoch 17/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 20962345.5667 - mean_squared_error: 20962144.0667 - val_loss: 20426298.9000 - val_mean_squared_error: 20426092.9000\n",
            "Epoch 18/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 19741801.6667 - mean_squared_error: 19741592.4667 - val_loss: 19083793.3000 - val_mean_squared_error: 19083579.3000\n",
            "Epoch 19/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 18459422.3833 - mean_squared_error: 18459205.2667 - val_loss: 17773579.6000 - val_mean_squared_error: 17773357.6000\n",
            "Epoch 20/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 17114465.8667 - mean_squared_error: 17114240.2667 - val_loss: 16406384.3500 - val_mean_squared_error: 16406154.3500\n",
            "Epoch 21/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 15738761.4500 - mean_squared_error: 15738527.1667 - val_loss: 15144406.8500 - val_mean_squared_error: 15144168.8500\n",
            "Epoch 22/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 14507214.7833 - mean_squared_error: 14506972.9667 - val_loss: 13823638.6000 - val_mean_squared_error: 13823393.6000\n",
            "Epoch 23/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 13093125.9500 - mean_squared_error: 13092877.4833 - val_loss: 12610412.8500 - val_mean_squared_error: 12610160.8500\n",
            "Epoch 24/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 11956644.3333 - mean_squared_error: 11956389.8000 - val_loss: 11369280.9500 - val_mean_squared_error: 11369022.9500\n",
            "Epoch 25/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 10710536.8500 - mean_squared_error: 10710276.6167 - val_loss: 10143019.6500 - val_mean_squared_error: 10142756.6500\n",
            "Epoch 26/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 9514276.2667 - mean_squared_error: 9514010.6750 - val_loss: 9005853.8500 - val_mean_squared_error: 9005585.8500\n",
            "Epoch 27/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 8548023.2917 - mean_squared_error: 8547752.7000 - val_loss: 7888859.9750 - val_mean_squared_error: 7888586.4750\n",
            "Epoch 28/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 7482211.7000 - mean_squared_error: 7481936.2333 - val_loss: 7052022.3500 - val_mean_squared_error: 7051744.8500\n",
            "Epoch 29/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 6613885.4250 - mean_squared_error: 6613605.8750 - val_loss: 6089185.2250 - val_mean_squared_error: 6088903.7250\n",
            "Epoch 30/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 5630381.4458 - mean_squared_error: 5630098.2667 - val_loss: 5167001.1000 - val_mean_squared_error: 5166716.1000\n",
            "Epoch 31/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 4858234.4208 - mean_squared_error: 4857947.8833 - val_loss: 4520251.7250 - val_mean_squared_error: 4519963.7250\n",
            "Epoch 32/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 4185836.6625 - mean_squared_error: 4185547.1792 - val_loss: 3896941.3500 - val_mean_squared_error: 3896650.6000\n",
            "Epoch 33/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 3465721.6292 - mean_squared_error: 3465429.3333 - val_loss: 3243970.7125 - val_mean_squared_error: 3243677.2125\n",
            "Epoch 34/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 3018606.3229 - mean_squared_error: 3018311.4312 - val_loss: 2641444.6000 - val_mean_squared_error: 2641148.3500\n",
            "Epoch 35/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2498967.0750 - mean_squared_error: 2498670.0042 - val_loss: 2172379.8625 - val_mean_squared_error: 2172081.6125\n",
            "Epoch 36/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2071892.6667 - mean_squared_error: 2071593.6812 - val_loss: 1783315.5813 - val_mean_squared_error: 1783015.5813\n",
            "Epoch 37/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 1658619.1552 - mean_squared_error: 1658318.3813 - val_loss: 1444359.8687 - val_mean_squared_error: 1444058.2437\n",
            "Epoch 38/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 1329704.8406 - mean_squared_error: 1329402.6698 - val_loss: 1185790.7063 - val_mean_squared_error: 1185487.8313\n",
            "Epoch 39/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 1057842.7167 - mean_squared_error: 1057539.1740 - val_loss: 925943.3344 - val_mean_squared_error: 925639.2719\n",
            "Epoch 40/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 911481.8562 - mean_squared_error: 911177.2714 - val_loss: 733104.9125 - val_mean_squared_error: 732799.7875\n",
            "Epoch 41/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 728894.6896 - mean_squared_error: 728589.0641 - val_loss: 572289.2828 - val_mean_squared_error: 571983.1594\n",
            "Epoch 42/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 604970.2010 - mean_squared_error: 604663.6854 - val_loss: 424657.4688 - val_mean_squared_error: 424350.5625\n",
            "Epoch 43/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 457782.9276 - mean_squared_error: 457475.6535 - val_loss: 347108.1594 - val_mean_squared_error: 346800.5031\n",
            "Epoch 44/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 365825.7296 - mean_squared_error: 365517.8072 - val_loss: 257834.6031 - val_mean_squared_error: 257526.3641\n",
            "Epoch 45/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 317892.6667 - mean_squared_error: 317584.1031 - val_loss: 186915.5398 - val_mean_squared_error: 186606.6648\n",
            "Epoch 46/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 267454.2549 - mean_squared_error: 267145.1068 - val_loss: 147381.3012 - val_mean_squared_error: 147071.9090\n",
            "Epoch 47/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 230172.4467 - mean_squared_error: 229862.8164 - val_loss: 106454.7406 - val_mean_squared_error: 106144.8500\n",
            "Epoch 48/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 208251.9194 - mean_squared_error: 207941.7214 - val_loss: 75102.1854 - val_mean_squared_error: 74791.6547\n",
            "Epoch 49/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 187456.8938 - mean_squared_error: 187146.1030 - val_loss: 53630.3971 - val_mean_squared_error: 53319.3775\n",
            "Epoch 50/50\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 186426.4316 - mean_squared_error: 186115.0782 - val_loss: 41413.6233 - val_mean_squared_error: 41101.9124\n",
            "200/200 [==============================] - 0s 819us/step\n",
            "acc :  39156.83447265625\n",
            "[[5490.2734]\n",
            " [5597.362 ]\n",
            " [5313.94  ]\n",
            " [5340.5107]\n",
            " [4954.3276]\n",
            " [5705.256 ]\n",
            " [5109.76  ]\n",
            " [5421.0283]\n",
            " [5268.0444]\n",
            " [5226.1753]\n",
            " [4971.3267]\n",
            " [4952.7085]\n",
            " [5269.655 ]\n",
            " [5589.31  ]\n",
            " [4969.7085]\n",
            " [5663.386 ]\n",
            " [5562.7393]\n",
            " [4915.3335]\n",
            " [5041.758 ]\n",
            " [5243.889 ]\n",
            " [5263.214 ]\n",
            " [5318.771 ]\n",
            " [5287.369 ]\n",
            " [5520.065 ]\n",
            " [5439.5474]\n",
            " [5282.5376]\n",
            " [5180.9995]\n",
            " [5484.637 ]\n",
            " [5108.14  ]\n",
            " [5444.3784]\n",
            " [5255.1616]\n",
            " [5116.2354]\n",
            " [5143.7603]\n",
            " [5393.652 ]\n",
            " [5291.394 ]\n",
            " [5258.3823]\n",
            " [4940.565 ]\n",
            " [4960.8037]\n",
            " [5706.061 ]\n",
            " [5657.7495]\n",
            " [5183.4277]\n",
            " [5223.7593]\n",
            " [5198.8   ]\n",
            " [5333.264 ]\n",
            " [5527.312 ]\n",
            " [4968.0894]\n",
            " [5160.7603]\n",
            " [5078.187 ]\n",
            " [5292.1997]\n",
            " [5619.9067]\n",
            " [5572.402 ]\n",
            " [5058.7583]\n",
            " [5454.845 ]\n",
            " [5395.2627]\n",
            " [5525.7017]\n",
            " [5678.685 ]\n",
            " [5115.427 ]\n",
            " [5535.364 ]\n",
            " [5267.2393]\n",
            " [5158.3325]\n",
            " [5456.4556]\n",
            " [4956.755 ]\n",
            " [5238.2524]\n",
            " [5542.61  ]\n",
            " [5526.5063]\n",
            " [5192.333 ]\n",
            " [5381.5747]\n",
            " [5068.473 ]\n",
            " [5182.6177]\n",
            " [5720.553 ]\n",
            " [4966.47  ]\n",
            " [5716.528 ]\n",
            " [5594.946 ]\n",
            " [5553.882 ]\n",
            " [5039.3296]\n",
            " [5074.1396]\n",
            " [5009.376 ]\n",
            " [5612.6606]\n",
            " [5401.7036]\n",
            " [5061.187 ]\n",
            " [5514.4287]\n",
            " [4922.986 ]\n",
            " [5648.894 ]\n",
            " [5095.1875]\n",
            " [5350.1724]\n",
            " [5379.9644]\n",
            " [5491.8843]\n",
            " [4923.8364]\n",
            " [5432.3003]\n",
            " [5684.322 ]\n",
            " [5220.5396]\n",
            " [5159.9517]\n",
            " [5531.3374]\n",
            " [5264.018 ]\n",
            " [5687.5415]\n",
            " [5491.078 ]\n",
            " [5273.681 ]\n",
            " [5414.587 ]\n",
            " [5170.475 ]\n",
            " [5316.356 ]\n",
            " [5060.3774]\n",
            " [5014.2334]\n",
            " [5615.8804]\n",
            " [5547.441 ]\n",
            " [5424.2485]\n",
            " [5247.9146]\n",
            " [5200.4097]\n",
            " [4989.947 ]\n",
            " [5008.5664]\n",
            " [5538.584 ]\n",
            " [5604.6084]\n",
            " [5621.518 ]\n",
            " [5043.3765]\n",
            " [5556.299 ]\n",
            " [5310.719 ]\n",
            " [5218.1235]\n",
            " [5631.1797]\n",
            " [5475.7803]\n",
            " [5643.2573]\n",
            " [5359.8345]\n",
            " [5569.181 ]\n",
            " [4959.9937]\n",
            " [5466.1187]\n",
            " [5236.643 ]\n",
            " [5254.3555]\n",
            " [5257.5767]\n",
            " [5347.7573]\n",
            " [5523.286 ]\n",
            " [5659.3604]\n",
            " [5092.759 ]\n",
            " [5372.7173]\n",
            " [5078.9966]\n",
            " [5032.0435]\n",
            " [5230.2017]\n",
            " [5066.0444]\n",
            " [5081.4253]\n",
            " [4995.614 ]\n",
            " [5153.474 ]\n",
            " [5293.81  ]\n",
            " [5226.98  ]\n",
            " [5134.855 ]\n",
            " [5611.855 ]\n",
            " [4944.6123]\n",
            " [5029.6147]\n",
            " [5474.9746]\n",
            " [5150.237 ]\n",
            " [5024.7573]\n",
            " [5151.8555]\n",
            " [5392.042 ]\n",
            " [5179.3804]\n",
            " [5210.8774]\n",
            " [5460.482 ]\n",
            " [5445.989 ]\n",
            " [5695.5938]\n",
            " [5671.438 ]\n",
            " [5049.853 ]\n",
            " [5686.7363]\n",
            " [5509.597 ]\n",
            " [5415.392 ]\n",
            " [5127.5693]\n",
            " [5084.6636]\n",
            " [5218.9287]\n",
            " [5677.074 ]\n",
            " [5564.35  ]\n",
            " [5447.5986]\n",
            " [5580.453 ]\n",
            " [5499.9355]\n",
            " [5461.2866]\n",
            " [5570.7915]\n",
            " [4919.585 ]\n",
            " [5210.072 ]\n",
            " [5106.521 ]\n",
            " [5098.426 ]\n",
            " [5594.141 ]\n",
            " [5497.52  ]\n",
            " [5191.523 ]\n",
            " [5644.062 ]\n",
            " [5335.6807]\n",
            " [5668.2173]\n",
            " [4993.185 ]\n",
            " [4934.0405]\n",
            " [5551.467 ]\n",
            " [5283.3433]\n",
            " [5276.096 ]\n",
            " [5645.673 ]\n",
            " [5708.476 ]\n",
            " [4955.946 ]\n",
            " [5246.305 ]\n",
            " [4976.185 ]\n",
            " [5408.146 ]\n",
            " [5454.041 ]\n",
            " [5498.3257]\n",
            " [5512.818 ]\n",
            " [5585.284 ]\n",
            " [5428.2744]\n",
            " [5486.2476]\n",
            " [4946.232 ]\n",
            " [5539.3896]\n",
            " [5205.2407]\n",
            " [5419.418 ]]\n",
            "RMSE :  197.88078014902595\n",
            "R2 :  0.5226143666646959\n",
            "loss :  39468.5453125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}